{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal, working PyTorch skeleton for:\n",
    "- Dual-stream FSCIL (Stable=EMA teacher, Plastic=student)\n",
    "- Prompt learning (base prompt + session prompt)\n",
    "- Global prototype memory (NCM classifier)\n",
    "- Knowledge Distillation (stable -> plastic) in prototype-logit space\n",
    "- Domain alignment loss (old vs new feature mean)\n",
    "- Base training + incremental sessions + inference\n",
    "\n",
    "Assumptions:\n",
    "- You already have dataloaders that yield (images, labels).\n",
    "- Labels are global class IDs (e.g., 0..C-1 across all sessions).\n",
    "- Backbone here is a simple ViT-like \"token\" encoder for clarity.\n",
    "  Replace PromptedBackbone with timm ViT (recommended) if you want.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utils\n",
    "# -----------------------------\n",
    "def l2_normalize(x: torch.Tensor, dim: int = -1, eps: float = 1e-12) -> torch.Tensor:\n",
    "    return x / (x.norm(p=2, dim=dim, keepdim=True) + eps)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(teacher: nn.Module, student: nn.Module, alpha: float = 0.999) -> None:\n",
    "    \"\"\"teacher = alpha*teacher + (1-alpha)*student\"\"\"\n",
    "    for (n_t, p_t), (n_s, p_s) in zip(teacher.named_parameters(), student.named_parameters()):\n",
    "        assert n_t == n_s, f\"Param mismatch: {n_t} vs {n_s}\"\n",
    "        p_t.data.mul_(alpha).add_(p_s.data, alpha=(1 - alpha))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt module\n",
    "# -----------------------------\n",
    "class PromptModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Prompts are learnable tokens with the SAME dim as backbone embedding dim D.\n",
    "    Shape: [m, D]  (m prompt tokens)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_prompts: int, dim: int, init_std: float = 0.02):\n",
    "        super().__init__()\n",
    "        self.prompts = nn.Parameter(torch.randn(num_prompts, dim) * init_std)\n",
    "\n",
    "    def forward(self, batch_size: int) -> torch.Tensor:\n",
    "        # Return prompts expanded to batch: [B, m, D]\n",
    "        return self.prompts.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# A small backbone (toy ViT-style token encoder)\n",
    "# Replace with timm ViT for real runs.\n",
    "# -----------------------------\n",
    "class TinyTokenBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a lightweight placeholder for demonstration.\n",
    "    For real FSCIL, replace with a real ViT (e.g., timm vit_base_patch16_224).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int = 3, dim: int = 768):\n",
    "        super().__init__()\n",
    "        # crude \"patch embedding\": global pooling + linear\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.proj = nn.Linear(in_ch, dim)\n",
    "        # a small MLP to mimic transformer depth\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "        self.out_norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B,3,H,W] -> [B, dim]\n",
    "        b, c, _, _ = x.shape\n",
    "        v = self.pool(x).view(b, c)           # [B,3]\n",
    "        h = self.proj(v)                      # [B,dim]\n",
    "        h = h + self.mlp(h)                   # residual-ish\n",
    "        h = self.out_norm(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class PromptedBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a backbone with prompt injection.\n",
    "    For real ViT:\n",
    "      - you would concat prompt tokens with patch tokens.\n",
    "    Here, we simulate prompt influence by projecting pooled prompt summary and adding to features.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone: nn.Module, prompt: PromptModule, dim: int = 768):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.prompt = prompt\n",
    "        self.prompt_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b = x.shape[0]\n",
    "        h = self.backbone(x)                  # [B, D]\n",
    "        p = self.prompt(b)                    # [B, m, D]\n",
    "        p_sum = p.mean(dim=1)                 # [B, D]\n",
    "        h = h + self.prompt_proj(p_sum)       # prompt influences features\n",
    "        return h\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prototype memory (global across sessions)\n",
    "# -----------------------------\n",
    "class PrototypeMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Stores class prototypes (centroids). Global, persistent.\n",
    "    - prototypes: [C, D]\n",
    "    - counts: [C] number of samples accumulated per class\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim: int):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.register_buffer(\"prototypes\", torch.empty(0, feat_dim))\n",
    "        self.register_buffer(\"counts\", torch.empty(0))\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        return int(self.prototypes.shape[0])\n",
    "\n",
    "    def ensure_size(self, num_classes: int, device: torch.device) -> None:\n",
    "        if self.num_classes >= num_classes:\n",
    "            return\n",
    "        old_C = self.num_classes\n",
    "        new_C = num_classes\n",
    "        # expand prototypes and counts\n",
    "        pad_proto = torch.zeros(new_C - old_C, self.feat_dim, device=device)\n",
    "        pad_cnt = torch.zeros(new_C - old_C, device=device)\n",
    "        if old_C == 0:\n",
    "            self.prototypes = pad_proto\n",
    "            self.counts = pad_cnt\n",
    "        else:\n",
    "            self.prototypes = torch.cat([self.prototypes.to(device), pad_proto], dim=0)\n",
    "            self.counts = torch.cat([self.counts.to(device), pad_cnt], dim=0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_from_batch(self, feats: torch.Tensor, labels: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Online mean update:\n",
    "          mu_new = (n*mu_old + sum(feats))/ (n + k)\n",
    "        feats: [B,D], labels: [B]\n",
    "        \"\"\"\n",
    "        device = feats.device\n",
    "        max_label = int(labels.max().item())\n",
    "        self.ensure_size(max_label + 1, device=device)\n",
    "\n",
    "        for c in labels.unique():\n",
    "            c = int(c.item())\n",
    "            idx = (labels == c)\n",
    "            f_c = feats[idx]  # [k,D]\n",
    "            k = f_c.shape[0]\n",
    "            if k == 0:\n",
    "                continue\n",
    "            mu_old = self.prototypes[c]\n",
    "            n_old = self.counts[c].clamp(min=0.0)\n",
    "            mu_new = (n_old * mu_old + f_c.sum(dim=0)) / (n_old + float(k))\n",
    "            self.prototypes[c] = mu_new\n",
    "            self.counts[c] = n_old + float(k)\n",
    "\n",
    "    def logits_ncm(self, feats: torch.Tensor, tau: float = 1.0, normalize: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Prototype-based logits:\n",
    "          logits_c = -||h - mu_c||^2 / tau\n",
    "        \"\"\"\n",
    "        if self.num_classes == 0:\n",
    "            raise RuntimeError(\"PrototypeMemory is empty. Build prototypes first (base session).\")\n",
    "        proto = self.prototypes.to(feats.device)\n",
    "        h = feats\n",
    "        if normalize:\n",
    "            proto = l2_normalize(proto, dim=-1)\n",
    "            h = l2_normalize(h, dim=-1)\n",
    "\n",
    "        # squared Euclidean distances\n",
    "        # dists: [B,C]\n",
    "        dists = torch.cdist(h, proto, p=2) ** 2\n",
    "        logits = -dists / tau\n",
    "        return logits\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Losses\n",
    "# -----------------------------\n",
    "def proto_alignment_loss(feats: torch.Tensor, labels: torch.Tensor, proto_mem: PrototypeMemory) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pull feature toward its class prototype: ||h - mu_y||^2\n",
    "    \"\"\"\n",
    "    proto = proto_mem.prototypes.to(feats.device)\n",
    "    mu = proto[labels]  # [B,D]\n",
    "    feats_n = l2_normalize(feats, dim=-1)\n",
    "    mu_n = l2_normalize(mu, dim=-1)\n",
    "    return ((feats_n - mu_n) ** 2).sum(dim=-1).mean()\n",
    "\n",
    "\n",
    "def kd_loss_logits(logits_teacher: torch.Tensor, logits_student: torch.Tensor, T: float = 2.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    KD on logits:\n",
    "      KL( softmax(teacher/T) || softmax(student/T) ) * T^2\n",
    "    \"\"\"\n",
    "    p_t = F.softmax(logits_teacher / T, dim=-1)\n",
    "    log_p_s = F.log_softmax(logits_student / T, dim=-1)\n",
    "    return F.kl_div(log_p_s, p_t, reduction=\"batchmean\") * (T * T)\n",
    "\n",
    "\n",
    "def domain_shift_loss(feats_old: torch.Tensor, feats_new: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Align mean features between old-domain (stable) and new-domain (plastic):\n",
    "      || mean(old) - mean(new) ||^2\n",
    "    \"\"\"\n",
    "    mu_old = feats_old.mean(dim=0)\n",
    "    mu_new = feats_new.mean(dim=0)\n",
    "    mu_old = l2_normalize(mu_old, dim=-1)\n",
    "    mu_new = l2_normalize(mu_new, dim=-1)\n",
    "    return ((mu_old - mu_new) ** 2).sum()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# FSCIL model wrapper\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class FSCILConfig:\n",
    "    feat_dim: int = 768\n",
    "    num_prompts: int = 10\n",
    "    tau: float = 1.0\n",
    "    kd_T: float = 2.0\n",
    "    ema_alpha: float = 0.999\n",
    "\n",
    "    # loss weights\n",
    "    lam_proto: float = 1.0\n",
    "    lam_kd: float = 1.0\n",
    "    lam_domain: float = 0.5\n",
    "\n",
    "\n",
    "class FSCILSystem(nn.Module):\n",
    "    \"\"\"\n",
    "    Holds:\n",
    "    - stable backbone + base prompt (teacher)\n",
    "    - plastic backbone + session prompt (student) during training\n",
    "    - global prototype memory\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: FSCILConfig, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "\n",
    "        # Base prompt (learned in base session)\n",
    "        self.base_prompt = PromptModule(cfg.num_prompts, cfg.feat_dim).to(device)\n",
    "\n",
    "        # Backbones\n",
    "        base_backbone = TinyTokenBackbone(dim=cfg.feat_dim).to(device)\n",
    "\n",
    "        # Plastic uses (backbone + session prompt) that will be re-created per session\n",
    "        self.plastic_prompt = None\n",
    "        self.plastic = None\n",
    "\n",
    "        # Stable uses (backbone + base prompt), EMA-updated\n",
    "        self.stable = PromptedBackbone(copy.deepcopy(base_backbone), self.base_prompt, dim=cfg.feat_dim).to(device)\n",
    "        for p in self.stable.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Prototype memory\n",
    "        self.proto_mem = PrototypeMemory(cfg.feat_dim).to(device)\n",
    "\n",
    "        # Keep a reference backbone init for creating new plastic streams\n",
    "        self._backbone_template = base_backbone\n",
    "\n",
    "    def new_plastic_for_session(self) -> None:\n",
    "        \"\"\"\n",
    "        Create a fresh plastic stream and session prompt for a new incremental session.\n",
    "        Plastic initialized from stable backbone weights.\n",
    "        \"\"\"\n",
    "        # Clone stable backbone weights into a trainable backbone\n",
    "        backbone = copy.deepcopy(self._backbone_template).to(self.device)\n",
    "\n",
    "        # Load stable's backbone weights (stable.backbone)\n",
    "        backbone.load_state_dict(self.stable.backbone.state_dict(), strict=True)\n",
    "\n",
    "        # Create session prompt initialized from base prompt (+ small noise)\n",
    "        P_base = self.base_prompt.prompts.detach()\n",
    "        session_prompt = PromptModule(self.cfg.num_prompts, self.cfg.feat_dim).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            session_prompt.prompts.copy_(P_base + 0.01 * torch.randn_like(P_base))\n",
    "\n",
    "        self.plastic_prompt = session_prompt\n",
    "        self.plastic = PromptedBackbone(backbone, self.plastic_prompt, dim=self.cfg.feat_dim).to(self.device)\n",
    "\n",
    "        # Plastic trainable\n",
    "        for p in self.plastic.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def build_prototypes_from_loader(self, loader) -> None:\n",
    "        \"\"\"\n",
    "        Compute/update prototypes using stable stream + base prompt (recommended).\n",
    "        \"\"\"\n",
    "        self.stable.eval()\n",
    "        for x, y in loader:\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            h = self.stable(x)  # [B,D]\n",
    "            self.proto_mem.update_from_batch(h, y)\n",
    "\n",
    "    def inference(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Final testing:\n",
    "        x -> stable (base prompt) -> NCM logits -> argmax\n",
    "        \"\"\"\n",
    "        self.stable.eval()\n",
    "        with torch.no_grad():\n",
    "            h = self.stable(x.to(self.device))\n",
    "            logits = self.proto_mem.logits_ncm(h, tau=self.cfg.tau, normalize=True)\n",
    "            pred = logits.argmax(dim=-1)\n",
    "        return pred\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training routines\n",
    "# -----------------------------\n",
    "def train_base_session(\n",
    "    sys: FSCILSystem,\n",
    "    base_loader,\n",
    "    epochs: int = 5,\n",
    "    lr: float = 1e-3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Base session:\n",
    "    - Train a plastic stream (trainable) + base prompt\n",
    "    - EMA-update stable stream\n",
    "    - Build prototypes at end\n",
    "    \"\"\"\n",
    "    cfg = sys.cfg\n",
    "    device = sys.device\n",
    "\n",
    "    # Create a trainable plastic that uses the BASE prompt directly in base session\n",
    "    backbone = copy.deepcopy(sys._backbone_template).to(device)\n",
    "    plastic = PromptedBackbone(backbone, sys.base_prompt, dim=cfg.feat_dim).to(device)\n",
    "    for p in plastic.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # Stable is frozen EMA teacher, initialized already\n",
    "    sys.stable.eval()\n",
    "\n",
    "    opt = torch.optim.AdamW(plastic.parameters(), lr=lr)\n",
    "\n",
    "    plastic.train()\n",
    "    for ep in range(epochs):\n",
    "        for x, y in base_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Forward (student)\n",
    "            h_p = plastic(x)\n",
    "            logits_p = sys.proto_mem.logits_ncm(h_p, tau=cfg.tau) if sys.proto_mem.num_classes > 0 else None\n",
    "\n",
    "            # For base session, if proto memory empty, we can do a temporary linear head\n",
    "            # Here we use a quick linear head for base training only.\n",
    "            if logits_p is None or logits_p.shape[1] <= int(y.max().item()):\n",
    "                # Make a temp head sized to current max label\n",
    "                C = int(y.max().item()) + 1\n",
    "                temp_head = nn.Linear(cfg.feat_dim, C).to(device)\n",
    "                temp_head.train()\n",
    "                opt_head = torch.optim.AdamW(temp_head.parameters(), lr=lr)\n",
    "\n",
    "                h_p2 = l2_normalize(h_p)\n",
    "                logits = temp_head(h_p2)\n",
    "                loss_cls = F.cross_entropy(logits, y)\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                opt_head.zero_grad(set_to_none=True)\n",
    "                loss_cls.backward()\n",
    "                opt.step()\n",
    "                opt_head.step()\n",
    "\n",
    "                # EMA update stable from plastic backbone+prompt parameters:\n",
    "                # stable's prompt is the same base prompt; stable backbone should follow plastic backbone.\n",
    "                ema_update(sys.stable.backbone, plastic.backbone, alpha=cfg.ema_alpha)\n",
    "\n",
    "            else:\n",
    "                # If prototypes exist (rare at start), you can use NCM directly\n",
    "                loss_cls = F.cross_entropy(logits_p, y)\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                loss_cls.backward()\n",
    "                opt.step()\n",
    "                ema_update(sys.stable.backbone, plastic.backbone, alpha=cfg.ema_alpha)\n",
    "\n",
    "        print(f\"[Base] epoch {ep+1}/{epochs} done.\")\n",
    "\n",
    "    # Build base prototypes using stable stream\n",
    "    sys.proto_mem.prototypes = sys.proto_mem.prototypes[:0]  # reset\n",
    "    sys.proto_mem.counts = sys.proto_mem.counts[:0]\n",
    "    sys.build_prototypes_from_loader(base_loader)\n",
    "    print(\"[Base] prototypes built:\", sys.proto_mem.num_classes)\n",
    "\n",
    "\n",
    "def train_incremental_session(\n",
    "    sys: FSCILSystem,\n",
    "    new_loader,\n",
    "    old_loader: Optional[object],  # can be None if you do not have replay\n",
    "    epochs: int = 5,\n",
    "    lr: float = 5e-4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Incremental session t:\n",
    "    - Create new plastic stream & session prompt\n",
    "    - Stable stream is frozen teacher\n",
    "    - Loss = cls + proto + kd + domain\n",
    "    - Update prototypes for new classes\n",
    "    - EMA merge plastic -> stable\n",
    "    \"\"\"\n",
    "    cfg = sys.cfg\n",
    "    device = sys.device\n",
    "\n",
    "    sys.new_plastic_for_session()\n",
    "    assert sys.plastic is not None\n",
    "\n",
    "    sys.stable.eval()\n",
    "    sys.plastic.train()\n",
    "\n",
    "    opt = torch.optim.AdamW(sys.plastic.parameters(), lr=lr)\n",
    "\n",
    "    # If you have replay, we iterate both loaders; otherwise KD/domain become weaker.\n",
    "    old_iter = iter(old_loader) if old_loader is not None else None\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        for x_new, y_new in new_loader:\n",
    "            x_new = x_new.to(device)\n",
    "            y_new = y_new.to(device)\n",
    "\n",
    "            # -------- new-class forward (plastic) --------\n",
    "            h_new = sys.plastic(x_new)  # [B,D]\n",
    "            logits_new = sys.proto_mem.logits_ncm(h_new, tau=cfg.tau, normalize=True)\n",
    "            loss_cls = F.cross_entropy(logits_new, y_new)\n",
    "\n",
    "            # Prototype alignment (only if prototype exists for those classes; after first update it will)\n",
    "            # For the first mini-batch of a brand-new class, prototypes might be zero -> still ok but weak.\n",
    "            loss_proto = proto_alignment_loss(h_new, y_new, sys.proto_mem)\n",
    "\n",
    "            # -------- old-class forward (stable teacher + plastic student) --------\n",
    "            loss_kd = torch.tensor(0.0, device=device)\n",
    "            loss_dom = torch.tensor(0.0, device=device)\n",
    "\n",
    "            if old_iter is not None:\n",
    "                try:\n",
    "                    x_old, y_old = next(old_iter)\n",
    "                except StopIteration:\n",
    "                    old_iter = iter(old_loader)\n",
    "                    x_old, y_old = next(old_iter)\n",
    "\n",
    "                x_old = x_old.to(device)\n",
    "                y_old = y_old.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    h_old_s = sys.stable(x_old)  # stable features\n",
    "                    logits_old_s = sys.proto_mem.logits_ncm(h_old_s, tau=cfg.tau, normalize=True)\n",
    "\n",
    "                h_old_p = sys.plastic(x_old)  # plastic features\n",
    "                logits_old_p = sys.proto_mem.logits_ncm(h_old_p, tau=cfg.tau, normalize=True)\n",
    "\n",
    "                # KD on prototype-logits (unique aspect)\n",
    "                loss_kd = kd_loss_logits(logits_old_s, logits_old_p, T=cfg.kd_T)\n",
    "\n",
    "                # Domain alignment: align mean(old stable) with mean(new plastic)\n",
    "                loss_dom = domain_shift_loss(h_old_s, h_new)\n",
    "\n",
    "            # -------- total --------\n",
    "            loss = loss_cls + cfg.lam_proto * loss_proto + cfg.lam_kd * loss_kd + cfg.lam_domain * loss_dom\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # EMA merge plastic -> stable backbone ONLY (stable prompt is base prompt, stays fixed)\n",
    "            ema_update(sys.stable.backbone, sys.plastic.backbone, alpha=cfg.ema_alpha)\n",
    "\n",
    "            # Update prototypes online using plastic features for new classes\n",
    "            with torch.no_grad():\n",
    "                sys.proto_mem.update_from_batch(h_new.detach(), y_new.detach())\n",
    "\n",
    "        print(f\"[Inc] epoch {ep+1}/{epochs} done.\")\n",
    "\n",
    "    # Discard plastic stream and session prompt to keep inference light\n",
    "    sys.plastic = None\n",
    "    sys.plastic_prompt = None\n",
    "    print(\"[Inc] session complete. total prototypes:\", sys.proto_mem.num_classes)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage (you plug your loaders)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    cfg = FSCILConfig(\n",
    "        feat_dim=768,          # ViT-B style\n",
    "        num_prompts=10,\n",
    "        tau=1.0,\n",
    "        kd_T=2.0,\n",
    "        ema_alpha=0.999,\n",
    "        lam_proto=1.0,\n",
    "        lam_kd=1.0,\n",
    "        lam_domain=0.5\n",
    "    )\n",
    "\n",
    "    sys = FSCILSystem(cfg, device=device)\n",
    "\n",
    "    # You must provide:\n",
    "    # base_loader: contains many images for base classes\n",
    "    # inc_loaders: list of loaders for each incremental session (few-shot new classes)\n",
    "    # old_replay_loader: optional, could be memory buffer loader\n",
    "\n",
    "    # train_base_session(sys, base_loader, epochs=50, lr=1e-4)\n",
    "    # for t, new_loader in enumerate(inc_loaders, start=1):\n",
    "    #     train_incremental_session(sys, new_loader, old_replay_loader, epochs=20, lr=5e-5)\n",
    "    #\n",
    "    # # Testing\n",
    "    # for x_test, y_test in test_loader:\n",
    "    #     pred = sys.inference(x_test)\n",
    "    #     ...\n",
    "\n",
    "    print(\"Skeleton ready. Plug your MiniImageNet loaders into the commented section.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiface_generate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
